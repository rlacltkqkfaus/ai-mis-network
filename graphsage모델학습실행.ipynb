{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ad68dcb",
   "metadata": {},
   "source": [
    "만든날짜: 0414  \n",
    "stellar_env 가상환경 사용. 파이썬 3.8버전  \n",
    "stellargraph를 불러오려면 chardet이라는 라이브러리가 별도로 필요함 -> 텐서플로우, 스텔라그래프, chardet 모두 설치  \n",
    "안됨 3.10버전에서 텐서플로우 2.9버전, 스텔라그래프 콘다 채널로 설치 + chardet (sage_env 가상환경)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9253fd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eric0\\AppData\\Local\\anaconda3\\envs\\sage_env\\lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.6 when it was built against 1.14.5, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StellarGraph version: 1.2.1\n",
      "TensorFlow version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "# --- StellarGraph 및 TensorFlow 임포트 ---\n",
    "\n",
    "try:\n",
    "    import stellargraph as sg\n",
    "    from stellargraph.mapper import GraphSAGENodeGenerator # GraphSAGENodeGenerator 임포트\n",
    "    from stellargraph.layer import GraphSAGE\n",
    "    print(f\"StellarGraph version: {sg.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"StellarGraph 또는 관련 모듈을 찾을 수 없습니다. 설치를 확인하세요.\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers, optimizers, losses, metrics, Model\n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"TensorFlow를 찾을 수 없습니다. 설치를 확인하세요.\")\n",
    "    exit()\n",
    "\n",
    "# (다른 필요한 라이브러리 import 계속...)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import io # 임시 데이터용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d353c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Executable: c:\\Users\\eric0\\AppData\\Local\\anaconda3\\envs\\sage_env\\python.exe\n",
      "Python Version: 3.10.13 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:15:57) [MSC v.1916 64 bit (AMD64)]\n",
      "TensorFlow Version: 2.9.1\n",
      "StellarGraph Version: 1.2.1\n",
      "임포트 테스트 완료\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"Python Executable: {sys.executable}\") # 현재 커널의 파이썬 경로 확인\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    print(f\"TensorFlow Version: {tf.__version__}\") # TF 임포트 및 버전 확인\n",
    "except Exception as e:\n",
    "    print(f\"TensorFlow 임포트 오류: {e}\")\n",
    "\n",
    "try:\n",
    "    import stellargraph as sg\n",
    "    print(f\"StellarGraph Version: {sg.__version__}\") # StellarGraph 임포트 및 버전 확인\n",
    "except Exception as e:\n",
    "    print(f\"StellarGraph 임포트 오류: {e}\")\n",
    "\n",
    "print(\"임포트 테스트 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d1f260b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba106d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_df = pd.read_csv(\"C:/Project/RAG연구동향분석/250409/전체기간(1-7)/전체기간(1-7)엣지.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94c899e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 node_df 로드 완료.\n",
      "로드 전 node_df info:\n",
      "\n",
      "Parquet 파일 'C:/Project/RAG연구동향분석/데이터/dblp_v14_filtered.parquet' 로드 완료.\n",
      "로드된 source_df info:\n",
      "\n",
      "source_df 컬럼 이름 변경 완료 (node_df와 맞추기 위해).\n",
      "변경된 source_df 컬럼: ['Id', 'Label', 'year', 'citations', 'venue', 'keywords', 'research_fields']\n",
      "\n",
      "'Id' 컬럼을 인덱스로 설정 완료.\n",
      "\n",
      "결측값 채우기 시작 (combine_first)...\n",
      "결측값 채우기 완료.\n",
      "\n",
      "결측값 채우기 후 node_df_filled info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5259862 entries, 53a725db20f7420be8b5bfc6 to 63ac3b54d88656000c1108fb\n",
      "Data columns (total 11 columns):\n",
      " #   Column            Dtype  \n",
      "---  ------            -----  \n",
      " 0   Degree            float64\n",
      " 1   Label             object \n",
      " 2   citations         float64\n",
      " 3   indegree          float64\n",
      " 4   keywords          object \n",
      " 5   level             object \n",
      " 6   modularity_class  float64\n",
      " 7   outdegree         float64\n",
      " 8   research_fields   object \n",
      " 9   venue             object \n",
      " 10  year              float64\n",
      "dtypes: float64(6), object(5)\n",
      "memory usage: 3.1 GB\n",
      "\n",
      "최종 데이터프레임 (node_df_final) info:\n",
      "\n",
      "'year' 컬럼 결측치 변화: 256841개 -> 5개\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "# --- 1. 데이터 로딩 ---\n",
    "try:\n",
    "    # 기존 노드 데이터 로드 (DtypeWarning 처리 위해 low_memory=False 추가)\n",
    "    node_df = pd.read_csv(\"C:/Project/RAG연구동향분석/250409/전체기간(1-7)/전체기간(1-7)노드.csv\", low_memory=False)\n",
    "    print(\"기존 node_df 로드 완료.\")\n",
    "    print(\"로드 전 node_df info:\")\n",
    "    # node_df.info(memory_usage='deep') # 필요시에만 info() 호출\n",
    "\n",
    "    # 전체 정보가 담긴 Parquet 파일 로드\n",
    "    parquet_path = 'C:/Project/RAG연구동향분석/데이터/dblp_v14_filtered.parquet'\n",
    "\n",
    "    # **** 중요: Parquet 파일의 실제 컬럼 이름과 대소문자까지 정확히 일치시키세요 ****\n",
    "    # 오류 메시지의 스키마 정보를 참고하여 수정 (예: Id -> id, Label -> title)\n",
    "    columns_to_read = [\n",
    "        'id',          # Parquet 파일의 ID 컬럼명 (소문자 'i')\n",
    "        'title',       # Parquet 파일의 제목 컬럼명 (Label 대신 title?)\n",
    "        'year',        # Parquet 파일의 연도 컬럼명\n",
    "        'n_citation',  # Parquet 파일의 인용수 컬럼명 (citations 대신 n_citation?)\n",
    "        'venue',       # Parquet 파일의 venue 컬럼명 (추가할 경우)\n",
    "        'keywords',    # Parquet 파일의 키워드 컬럼명 (list 형태일 수 있음)\n",
    "        'fos.name',    # Parquet 파일의 연구분야 컬럼명 (research_fields 대신 fos.name?)\n",
    "        # 'level' 컬럼은 Parquet 스키마에 없어 보이므로 제외, 필요시 확인/추가\n",
    "    ]\n",
    "    source_df = pd.read_parquet(parquet_path, columns=columns_to_read)\n",
    "    print(f\"\\nParquet 파일 '{parquet_path}' 로드 완료.\")\n",
    "    print(\"로드된 source_df info:\")\n",
    "    # source_df.info(memory_usage='deep') # 필요시에만 info() 호출\n",
    "\n",
    "    # **** Parquet 데이터 컬럼 이름 변경 (node_df와 맞추기 위해, 필요시) ****\n",
    "    # 예: source_df의 'id'를 'Id'로, 'title'을 'Label'로 변경 등\n",
    "    source_df.rename(columns={\n",
    "        'id': 'Id',\n",
    "        'title': 'Label',\n",
    "        'n_citation': 'citations',\n",
    "        'fos.name': 'research_fields',\n",
    "        # 'venue': 'venue_from_source', # 이름 충돌 방지 예시\n",
    "        # 'level'은 source_df에 없으므로 rename 대상 아님\n",
    "    }, inplace=True)\n",
    "    print(\"\\nsource_df 컬럼 이름 변경 완료 (node_df와 맞추기 위해).\")\n",
    "    print(f\"변경된 source_df 컬럼: {source_df.columns.tolist()}\")\n",
    "\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"파일 로딩 오류: {e}\")\n",
    "    print(\"파일 경로를 확인하세요.\")\n",
    "    exit()\n",
    "except ImportError:\n",
    "    print(\"오류: Parquet 파일을 읽기 위해 'pyarrow' 또는 'fastparquet' 라이브러리가 필요합니다.\")\n",
    "    print(\"pip install pandas pyarrow 또는 pip install pandas fastparquet 명령어로 설치하세요.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"데이터 로딩 중 오류 발생: {e}\")\n",
    "    exit() # 로딩 실패 시 중단\n",
    "\n",
    "# --- 2. 데이터 전처리 (ID를 인덱스로 설정) ---\n",
    "try:\n",
    "    if 'Id' not in node_df.columns or 'Id' not in source_df.columns:\n",
    "        print(\"오류: 'Id' 컬럼이 node_df 또는 source_df에 없습니다. (rename 후 확인)\")\n",
    "        exit()\n",
    "\n",
    "    # ID를 인덱스로 설정 (각 DataFrame의 'Id' 컬럼 사용)\n",
    "    # 원본을 유지하기 위해 새로운 변수에 할당\n",
    "    node_df_indexed = node_df.set_index('Id')\n",
    "    source_df_indexed = source_df.set_index('Id')\n",
    "\n",
    "    # 인덱스 타입 통일 시도 (문제가 없다면 생략 가능)\n",
    "    # node_df_indexed.index = node_df_indexed.index.astype(str)\n",
    "    # source_df_indexed.index = source_df_indexed.index.astype(str)\n",
    "    print(\"\\n'Id' 컬럼을 인덱스로 설정 완료.\")\n",
    "\n",
    "except KeyError as e:\n",
    "     print(f\"오류: 'Id' 컬럼({e})을 인덱스로 설정하는 데 실패했습니다. 컬럼 이름을 확인하세요.\")\n",
    "     exit() # 인덱스 설정 실패 시 중단\n",
    "except Exception as e:\n",
    "     print(f\"인덱스 설정 중 오류: {e}\")\n",
    "     exit() # 인덱스 설정 실패 시 중단\n",
    "\n",
    "\n",
    "# --- 3. combine_first를 사용하여 결측값 채우기 ---\n",
    "# source_df_indexed가 정상적으로 생성되었는지 확인 후 진행\n",
    "if 'source_df_indexed' in locals() and 'node_df_indexed' in locals():\n",
    "    print(\"\\n결측값 채우기 시작 (combine_first)...\")\n",
    "\n",
    "    # node_df_indexed의 NaN 값을 source_df_indexed의 값으로 채움\n",
    "    node_df_filled = node_df_indexed.combine_first(source_df_indexed)\n",
    "\n",
    "    print(\"결측값 채우기 완료.\")\n",
    "\n",
    "    # --- 4. 결과 확인 ---\n",
    "    print(\"\\n결측값 채우기 후 node_df_filled info:\")\n",
    "    node_df_filled.info(memory_usage='deep')\n",
    "\n",
    "    # (선택 사항) 인덱스를 다시 컬럼으로 되돌리기\n",
    "    node_df_final = node_df_filled.reset_index()\n",
    "    print(\"\\n최종 데이터프레임 (node_df_final) info:\")\n",
    "    # node_df_final.info(memory_usage='deep') # 너무 길면 주석 처리\n",
    "\n",
    "    # (선택 사항) 결측치가 얼마나 채워졌는지 확인 (예: 'year' 컬럼)\n",
    "    # combine_first는 원본 컬럼을 덮어쓰므로 node_df_filled에서 확인\n",
    "    original_nan_count = node_df['year'].isnull().sum() # 원본 node_df 기준\n",
    "    filled_nan_count = node_df_filled['year'].isnull().sum()\n",
    "    print(f\"\\n'year' 컬럼 결측치 변화: {original_nan_count}개 -> {filled_nan_count}개\")\n",
    "\n",
    "    # 이제 node_df_filled 또는 node_df_final을 사용하여 GraphSAGE 특징 생성 등을 진행할 수 있습니다.\n",
    "    # 예: df = node_df_final # GraphSAGE 코드에서 사용할 변수명으로 할당\n",
    "\n",
    "else:\n",
    "    print(\"\\n오류: source_df_indexed 또는 node_df_indexed가 정의되지 않아 combine_first를 실행할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f15d554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장 대상: node_df_final (Id 컬럼 포함)\n",
      "\n",
      "'C:/Project/RAG연구동향분석/250409/Preprocessed_Nodes\\nodes_filled_final.parquet' 경로에 데이터 저장 시작...\n",
      "파일 저장 중 오류 발생: ('cannot mix list and non-list, non-null values', 'Conversion failed for column keywords with type object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 사용자 설정: 저장할 파일 경로 ---\n",
    "# 저장할 디렉토리와 파일 이름을 지정합니다. (확장자를 .csv로 변경)\n",
    "output_dir_save = \"C:/Project/RAG연구동향분석/250409/Preprocessed_Nodes\"\n",
    "output_filename_csv = \"nodes_filled_final.csv\" # 파일 이름 변경\n",
    "output_path_csv = os.path.join(output_dir_save, output_filename_csv)\n",
    "\n",
    "# 저장할 디렉토리가 없으면 생성\n",
    "os.makedirs(output_dir_save, exist_ok=True)\n",
    "# ------------------------------------\n",
    "\n",
    "# --- 저장할 데이터프레임 확인 및 변환 ---\n",
    "dataframe_to_save = None\n",
    "if 'node_df_final' in locals() and isinstance(node_df_final, pd.DataFrame) and 'Id' in node_df_final.columns:\n",
    "    # node_df_final은 이미 'Id' 컬럼을 가지고 있다고 가정\n",
    "    dataframe_to_save = node_df_final\n",
    "    print(\"저장 대상: node_df_final (Id 컬럼 포함)\")\n",
    "elif 'node_df_filled' in locals() and isinstance(node_df_filled, pd.DataFrame) and node_df_filled.index.name == 'Id':\n",
    "    # node_df_filled는 'Id'가 인덱스이므로 컬럼으로 변환\n",
    "    dataframe_to_save = node_df_filled.reset_index() # reset_index()로 'Id'를 컬럼으로 만듦\n",
    "    print(\"저장 대상: node_df_filled (Id를 컬럼으로 변환하여 저장)\")\n",
    "else:\n",
    "    print(\"오류: 저장할 데이터프레임('node_df_final' 또는 'node_df_filled' - Id 포함)을 찾을 수 없습니다.\")\n",
    "    exit() # 저장할 데이터 없으면 중단\n",
    "\n",
    "# --- 데이터프레임 저장 ---\n",
    "try:\n",
    "    output_dir_save = \"C:/Project/RAG연구동향분석/250409/Preprocessed_Nodes\"\n",
    "    output_filename = \"nodes_filled_final.parquet\"\n",
    "    output_path = os.path.join(output_dir_save, output_filename)\n",
    "    os.makedirs(output_dir_save, exist_ok=True)\n",
    "\n",
    "    print(f\"\\n'{output_path}' 경로에 데이터 저장 시작...\")\n",
    "    # index=False 옵션으로 저장 (RangeIndex 저장 방지, 'Id'는 이미 컬럼임)\n",
    "    dataframe_to_save.to_parquet(output_path, index=False)\n",
    "    print(f\"저장 완료: {output_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"파일 저장 중 오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914a37bf",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7409281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "import re\n",
    "import io # 파일 로딩 에러 시 임시 데이터용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85c77539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "노드 및 엣지 데이터 준비 완료 (또는 로딩 완료).\n"
     ]
    }
   ],
   "source": [
    "df = node_df_final\n",
    "\n",
    "# --- 1. 데이터 로딩 ---\n",
    "# **** 중요: 이전 단계에서 결측치를 채운 node_df_final 변수를 사용한다고 가정 ****\n",
    "# 만약 변수명이 다르거나 로딩이 필요하면 이 부분을 수정하세요.\n",
    "# df = node_df_final # node_df_final 이 이미 로드되어 있다고 가정\n",
    "\n",
    "# 엣지 데이터 로드\n",
    "try:\n",
    "    # 실제 파일 경로로 수정하세요\n",
    "    # 만약 df = node_df_final 이 아니라면, 여기서 node_df도 로드해야 합니다.\n",
    "    # 예시: node_df = pd.read_csv(\"C:/Project/RAG연구동향분석/250409/전체기간(1-7)/전체기간(1-7)노드.csv\", low_memory=False)\n",
    "    #      df = node_df # node_df_final 대신 원본 사용 시\n",
    "\n",
    "    edges_df = pd.read_csv(\"C:/Project/RAG연구동향분석/250409/전체기간(1-7)/전체기간(1-7)엣지.csv\")\n",
    "    print(\"노드 및 엣지 데이터 준비 완료 (또는 로딩 완료).\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"파일 로딩 오류: {e}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"데이터 로딩 중 예상치 못한 오류 발생: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f0536ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. 사용자 설정 변수 ---\n",
    "YEAR_COLUMN = 'year'\n",
    "TARGET_COLUMN = 'modularity_class'\n",
    "TEXT_FEATURE_COLS = ['keywords', 'research_fields', 'Label'] # 특징으로 사용할 컬럼 (실제 df에 있는지 확인 필요)\n",
    "ID_COLUMN = 'Id' # 노드 ID 컬럼 이름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fd26805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. 데이터 기본 전처리 ---\n",
      "현재 df 인덱스 타입: <class 'pandas.core.indexes.base.Index'>, 인덱스 이름: None\n",
      "'Id' 컬럼을 인덱스로 설정했습니다. 새 인덱스 타입: object\n",
      "데이터 기본 전처리 완료.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. 데이터 기본 전처리 ---\n",
    "print(\"\\n--- 3. 데이터 기본 전처리 ---\")\n",
    "\n",
    "# 사용자 설정 변수 정의 (코드 상단 또는 이 섹션 시작 부분에 위치)\n",
    "YEAR_COLUMN = 'year'\n",
    "TARGET_COLUMN = 'modularity_class'\n",
    "ID_COLUMN = 'Id' # 노드 ID 컬럼 이름 정의\n",
    "\n",
    "# 필요한 컬럼 존재 확인\n",
    "required_cols_check = [YEAR_COLUMN, TARGET_COLUMN, ID_COLUMN]\n",
    "if not all(col in df.columns for col in required_cols_check):\n",
    "    print(f\"오류: 필요한 컬럼({required_cols_check}) 중 일부가 df에 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "# year 컬럼 숫자형 변환 및 결측치 처리\n",
    "df[YEAR_COLUMN] = pd.to_numeric(df[YEAR_COLUMN], errors='coerce')\n",
    "df.dropna(subset=[YEAR_COLUMN], inplace=True)\n",
    "df[YEAR_COLUMN] = df[YEAR_COLUMN].astype(int)\n",
    "\n",
    "# target 컬럼 숫자형 변환 및 결측치 처리\n",
    "df[TARGET_COLUMN] = pd.to_numeric(df[TARGET_COLUMN], errors='coerce').fillna(-1).astype(int)\n",
    "\n",
    "# --- 노드 ID 컬럼을 인덱스로 설정 (한 번만 실행) ---\n",
    "print(f\"현재 df 인덱스 타입: {type(df.index)}, 인덱스 이름: {df.index.name}\")\n",
    "if isinstance(df.index, pd.RangeIndex) and ID_COLUMN in df.columns:\n",
    "     df.set_index(ID_COLUMN, inplace=True) # 'Id' 컬럼을 인덱스로 설정!\n",
    "     print(f\"'{ID_COLUMN}' 컬럼을 인덱스로 설정했습니다. 새 인덱스 타입: {df.index.dtype}\")\n",
    "elif df.index.name == ID_COLUMN:\n",
    "     print(f\"'{ID_COLUMN}'이 이미 인덱스로 설정되어 있습니다.\")\n",
    "else:\n",
    "     # RangeIndex도 아니고, 이름이 ID_COLUMN인 인덱스도 아닌 경우\n",
    "     # -> ID_COLUMN이 존재하면 인덱스로 설정 시도, 없으면 오류\n",
    "     if ID_COLUMN in df.columns:\n",
    "         try:\n",
    "             df.set_index(ID_COLUMN, inplace=True)\n",
    "             print(f\"'{ID_COLUMN}' 컬럼을 인덱스로 설정했습니다. 새 인덱스 타입: {df.index.dtype}\")\n",
    "         except KeyError:\n",
    "              print(f\"오류: '{ID_COLUMN}' 컬럼을 인덱스로 설정하는 데 실패했습니다.\")\n",
    "              exit()\n",
    "     else:\n",
    "         print(f\"오류: '{ID_COLUMN}' 컬럼이 df에 없고, 현재 인덱스도 '{ID_COLUMN}'이 아닙니다.\")\n",
    "         exit() # ID 컬럼 없으면 중단\n",
    "# --- 인덱스 설정 끝 ---\n",
    "\n",
    "print(\"데이터 기본 전처리 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a95db5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. 텍스트 특징 생성 ---\n",
      "사용할 텍스트 특징 컬럼: ['keywords', 'research_fields', 'Label']\n",
      "Corpus 생성 완료 (총 5259857개 문서).\n",
      "TF-IDF 특징 생성 완료 (희소 행렬). Shape: (5259857, 100)\n",
      "밀집 배열로 변환 완료. Shape: (5259857, 100)\n",
      "최종 노드 특징 (밀집 배열) 준비 완료. Shape: (5259857, 100)\n"
     ]
    }
   ],
   "source": [
    "# --- 4. 텍스트 특징 생성 (TF-IDF) ---\n",
    "print(\"\\n--- 4. 텍스트 특징 생성 ---\")\n",
    "corpus = []\n",
    "actual_text_cols = [col for col in TEXT_FEATURE_COLS if col in df.columns] # 실제 존재하는 컬럼만 사용\n",
    "print(f\"사용할 텍스트 특징 컬럼: {actual_text_cols}\")\n",
    "\n",
    "# (V3 Corpus 생성 루프는 이전과 동일하게 사용)\n",
    "for index, row in df.iterrows(): # index는 'Id'\n",
    "    text_parts = []\n",
    "    for col in actual_text_cols:\n",
    "        cell_value = row[col]\n",
    "        try:\n",
    "            if isinstance(cell_value, str):\n",
    "                cleaned_value = cell_value.strip()\n",
    "                if cleaned_value: text_parts.append(cleaned_value)\n",
    "            elif isinstance(cell_value, (list, tuple, np.ndarray, pd.Series)):\n",
    "                list_parts = []\n",
    "                for item in cell_value:\n",
    "                    if pd.notna(item) and isinstance(item, str):\n",
    "                        cleaned_item = item.strip()\n",
    "                        if cleaned_item: list_parts.append(cleaned_item)\n",
    "                if list_parts: text_parts.append(\" \".join(list_parts))\n",
    "            elif pd.notna(cell_value):\n",
    "                cleaned_value = str(cell_value).strip()\n",
    "                if cleaned_value: text_parts.append(cleaned_value)\n",
    "        except Exception as e:\n",
    "             print(f\"  - Warning: Error processing cell - Index: {index}, Column: {col}, Value Type: {type(cell_value)}, Error: {e}\")\n",
    "             pass\n",
    "    corpus.append(\" \".join(text_parts))\n",
    "\n",
    "print(f\"Corpus 생성 완료 (총 {len(corpus)}개 문서).\")\n",
    "\n",
    "\n",
    "if corpus:\n",
    "    # **** max_features 값을 대폭 줄임 (예: 100) ****\n",
    "    vectorizer = TfidfVectorizer(max_features=100)\n",
    "    tfidf_features_sparse = vectorizer.fit_transform(corpus)\n",
    "    print(f\"TF-IDF 특징 생성 완료 (희소 행렬). Shape: {tfidf_features_sparse.shape}\")\n",
    "\n",
    "    # **** .toarray() 다시 사용! ****\n",
    "    try:\n",
    "        node_features = tfidf_features_sparse.toarray()\n",
    "        print(f\"밀집 배열로 변환 완료. Shape: {node_features.shape}\")\n",
    "    except MemoryError:\n",
    "         print(\"오류: max_features를 줄였음에도 MemoryError 발생! 더 줄이거나 다른 방법 필요.\")\n",
    "         exit()\n",
    "    except Exception as e:\n",
    "         print(f\"밀집 배열 변환 중 오류: {e}\")\n",
    "         exit()\n",
    "else:\n",
    "    # **** corpus가 비어있을 경우 처리 ****\n",
    "    print(\"경고: 텍스트 특징으로 사용할 Corpus가 비어있습니다.\")\n",
    "    # 이 경우 모델 학습에 특징을 사용할 수 없으므로 처리가 필요합니다.\n",
    "    # 옵션 1: 모든 특징을 0으로 채운 임시 특징 생성 (차원 수는 맞춰야 함)\n",
    "    # num_features = 100 # max_features와 동일하게 설정\n",
    "    # if 'df' in locals() and isinstance(df, pd.DataFrame):\n",
    "    #      node_features = np.zeros((len(df), num_features))\n",
    "    #      print(f\"임시 특징(0) 생성 완료. Shape: {node_features.shape}\")\n",
    "    # else:\n",
    "    #      print(\"오류: df 변수를 찾을 수 없어 임시 특징을 생성할 수 없습니다.\")\n",
    "    #      node_features = None # node_features를 None으로 유지\n",
    "\n",
    "    # 옵션 2: 특징 없이 진행하도록 None 유지 (이후 코드에서 None 처리 필요)\n",
    "    node_features = None\n",
    "    print(\"node_features를 None으로 설정합니다.\")\n",
    "\n",
    "# --- 최종 노드 특징 준비 ---\n",
    "if node_features is not None: # <--- 기준 라인\n",
    "    # **** 아래 코드 블록 전체가 들여쓰기 되어야 함 ****\n",
    "    try: # <--- if 보다 들여쓰기 됨\n",
    "        # df의 인덱스 상태 확인 및 node_feature_df 생성\n",
    "        if 'Id' in df.columns and isinstance(df.index, pd.RangeIndex): # <--- try 보다 들여쓰기 됨\n",
    "             node_feature_df = pd.DataFrame(node_features, index=df['Id'])\n",
    "             df.set_index('Id', inplace=True)\n",
    "        elif df.index.name == 'Id': # <--- try 보다 들여쓰기 됨\n",
    "             node_feature_df = pd.DataFrame(node_features, index=df.index)\n",
    "        # ... (이하 try 블록 내 다른 코드들도 동일하게 들여쓰기) ...\n",
    "\n",
    "        print(f\"최종 노드 특징 (밀집 배열) 준비 완료. Shape: {node_feature_df.shape}\")\n",
    "\n",
    "    except Exception as e: # <--- try 와 같은 수준으로 들여쓰기 됨\n",
    "        print(f\"노드 특징 DataFrame 생성 중 오류: {e}\") # <--- except 보다 들여쓰기 됨\n",
    "        exit()\n",
    "else: # <--- if 와 같은 수준으로 들여쓰기 됨\n",
    "    print(\"오류: node_features가 None입니다.\") # <--- else 보다 들여쓰기 됨\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e239db09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 5. 훈련/예측 대상 노드 ID 분리 ---\n",
      "훈련 노드 수 (Period 1-6): 4420308\n",
      "예측 대상 노드 수 (Period 7): 837944\n"
     ]
    }
   ],
   "source": [
    "# --- 5. 훈련/예측 대상 노드 ID 분리 ---\n",
    "print(\"\\n--- 5. 훈련/예측 대상 노드 ID 분리 ---\")\n",
    "period_6_end_year = 2019\n",
    "period_7_start_year = 2020\n",
    "period_7_end_year = 2022\n",
    "\n",
    "# df의 인덱스('Id')를 기준으로 필터링\n",
    "train_node_ids = df[df[YEAR_COLUMN] <= period_6_end_year].index\n",
    "predict_node_ids = df[(df[YEAR_COLUMN] >= period_7_start_year) & (df[YEAR_COLUMN] <= period_7_end_year)].index\n",
    "print(f\"훈련 노드 수 (Period 1-6): {len(train_node_ids)}\")\n",
    "print(f\"예측 대상 노드 수 (Period 7): {len(predict_node_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fc46b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 6. 훈련 레이블 준비 ---\n",
      "훈련 레이블 One-Hot 인코딩 완료. 클래스 수: 62\n",
      "클래스 매핑: {0: -1, 1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7, 9: 8, 10: 9, 11: 10, 12: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16, 18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23, 25: 24, 26: 25, 27: 26, 28: 27, 29: 28, 30: 29, 31: 30, 32: 31, 33: 32, 34: 33, 35: 34, 36: 35, 37: 36, 38: 37, 39: 38, 40: 39, 41: 40, 42: 41, 43: 42, 44: 43, 45: 44, 46: 45, 47: 46, 48: 47, 49: 48, 50: 49, 51: 50, 52: 51, 53: 52, 54: 53, 55: 54, 56: 55, 57: 56, 58: 57, 59: 58, 60: 60, 61: 61}\n"
     ]
    }
   ],
   "source": [
    "# --- 6. 훈련 레이블 준비 (One-Hot Encoding) ---\n",
    "print(\"\\n--- 6. 훈련 레이블 준비 ---\")\n",
    "if len(train_node_ids) > 0:\n",
    "    train_labels_raw = df.loc[train_node_ids, TARGET_COLUMN]\n",
    "    if train_labels_raw.nunique() <= 1:\n",
    "         print(\"오류: 훈련 데이터에 레이블 종류가 1개 이하입니다. 분류 모델 학습 불가.\")\n",
    "         exit()\n",
    "\n",
    "    target_encoder = LabelBinarizer()\n",
    "    train_targets = target_encoder.fit_transform(train_labels_raw)\n",
    "    num_classes = train_targets.shape[1]\n",
    "    print(f\"훈련 레이블 One-Hot 인코딩 완료. 클래스 수: {num_classes}\")\n",
    "    target_label_map = {i: label for i, label in enumerate(target_encoder.classes_)}\n",
    "    print(f\"클래스 매핑: {target_label_map}\")\n",
    "else:\n",
    "    print(\"오류: 훈련 데이터가 없습니다.\")\n",
    "    # 필요한 변수들 초기화 또는 exit()\n",
    "    train_targets = None\n",
    "    target_encoder = None\n",
    "    target_label_map = None\n",
    "    num_classes = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64018f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ID 일관성 확인 시작 ---\n",
      "df 인덱스 이름: Id\n",
      "df 인덱스 타입: object\n",
      "df 인덱스 샘플 (상위 5개): ['53a725db20f7420be8b5bfc6', '53a725db20f7420be8b5bff3', '53a725db20f7420be8b5bffb', '53a725db20f7420be8b5c00f', '53a727dc20f7420be8b9eed7']\n",
      "\n",
      "edges_df['Source'] 타입: object\n",
      "edges_df['Source'] 샘플 (상위 5개): ['53e9979bb7602d9701f66d44', '53e9979bb7602d9701f66d44', '53e9979bb7602d9701f66d44', '53e9979bb7602d9701f66d44', '53e9979bb7602d9701f66d44']\n",
      "\n",
      "edges_df['Target'] 타입: object\n",
      "edges_df['Target'] 샘플 (상위 5개): ['53e9979bb7602d9701f66d44', '53e9981db7602d970203b7b2', '53e99a98b7602d97023101c5', '53e9ab3eb7602d97034d0b36', '53e9ab69b7602d9703504883']\n",
      "\n",
      "첫 번째 엣지의 Source ID: '53e9979bb7602d9701f66d44' (타입: <class 'str'>)\n",
      "-> 이 ID는 df 인덱스에 존재합니다.\n",
      "--- ID 일관성 확인 끝 ---\n",
      "\n",
      "--- 7. 엣지 데이터 필터링 ---\n",
      "필터링 후 엣지 수: 1391357 (원본: 1391367, 제거됨: 10)\n",
      "\n",
      "--- 데이터 준비 완료 ---\n"
     ]
    }
   ],
   "source": [
    "# --- 7. 엣지 데이터 필터링 --- (전에 실행하여 확인)\n",
    "\n",
    "print(\"\\n--- ID 일관성 확인 시작 ---\")\n",
    "\n",
    "# df의 인덱스 정보 확인\n",
    "print(f\"df 인덱스 이름: {df.index.name}\")\n",
    "print(f\"df 인덱스 타입: {df.index.dtype}\")\n",
    "print(f\"df 인덱스 샘플 (상위 5개): {df.index[:5].tolist()}\") # 샘플 확인\n",
    "\n",
    "# edges_df의 Source/Target 컬럼 정보 확인\n",
    "if 'Source' in edges_df.columns and 'Target' in edges_df.columns:\n",
    "    print(f\"\\nedges_df['Source'] 타입: {edges_df['Source'].dtype}\")\n",
    "    print(f\"edges_df['Source'] 샘플 (상위 5개): {edges_df['Source'].head().tolist()}\")\n",
    "    print(f\"\\nedges_df['Target'] 타입: {edges_df['Target'].dtype}\")\n",
    "    print(f\"edges_df['Target'] 샘플 (상위 5개): {edges_df['Target'].head().tolist()}\")\n",
    "\n",
    "    # 샘플 ID 직접 비교 (첫 번째 엣지의 Source ID가 df 인덱스에 있는지 확인)\n",
    "    if not edges_df.empty:\n",
    "        sample_edge_source_id = edges_df['Source'].iloc[0]\n",
    "        # 타입을 문자열로 통일하여 비교 준비\n",
    "        sample_edge_source_id_str = str(sample_edge_source_id)\n",
    "        df_index_str_set = set(df.index.astype(str)) # df 인덱스도 문자열 Set으로\n",
    "\n",
    "        print(f\"\\n첫 번째 엣지의 Source ID: '{sample_edge_source_id}' (타입: {type(sample_edge_source_id)})\")\n",
    "        if sample_edge_source_id_str in df_index_str_set:\n",
    "            print(f\"-> 이 ID는 df 인덱스에 존재합니다.\")\n",
    "        else:\n",
    "            print(f\"-> **** 이 ID가 df 인덱스에 존재하지 않습니다! ****\")\n",
    "            # 존재하지 않는 이유 추적: 공백 문제? 대소문자 문제? 아예 없는 ID?\n",
    "            # 예: print(f\"   '{sample_edge_source_id_str.strip()}' in df_index_str_set ? {sample_edge_source_id_str.strip() in df_index_str_set}\")\n",
    "            # 예: print(f\"   '{sample_edge_source_id_str.lower()}' in set(idx.lower() for idx in df_index_str_set) ? {sample_edge_source_id_str.lower() in set(idx.lower() for idx in df_index_str_set)}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n오류: edges_df에 'Source' 또는 'Target' 컬럼이 없습니다.\")\n",
    "\n",
    "print(\"--- ID 일관성 확인 끝 ---\")\n",
    "\n",
    "\n",
    "\n",
    "# --- 7. 엣지 데이터 필터링 ---\n",
    "print(\"\\n--- 7. 엣지 데이터 필터링 ---\")\n",
    "valid_node_ids = set(df.index.astype(str)) # 모든 노드 ID (문자열로 변환하여 비교)\n",
    "original_edge_count = len(edges_df)\n",
    "\n",
    "# Source와 Target 노드가 모두 valid_node_ids에 존재하는 엣지만 남김 (타입 통일)\n",
    "edges_df_filtered = edges_df[\n",
    "    edges_df['Source'].astype(str).isin(valid_node_ids) &\n",
    "    edges_df['Target'].astype(str).isin(valid_node_ids)\n",
    "].copy()\n",
    "\n",
    "filtered_edge_count = len(edges_df_filtered)\n",
    "print(f\"필터링 후 엣지 수: {filtered_edge_count} (원본: {original_edge_count}, 제거됨: {original_edge_count - filtered_edge_count})\")\n",
    "print(\"\\n--- 데이터 준비 완료 ---\")\n",
    "\n",
    "# 이제 'node_feature_df', 'edges_df_filtered', 'train_node_ids',\n",
    "# 'predict_node_ids', 'train_targets', 'target_encoder', 'target_label_map', 'num_classes'\n",
    "# 변수들을 사용하여 StellarGraph 객체 생성 및 모델 빌드/학습을 진행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62538cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StellarGraph version: 1.2.1\n",
      "TensorFlow version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "# --- 필요한 라이브러리 임포트 (이미 임포트했다면 생략 가능) ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "try:\n",
    "    import stellargraph as sg\n",
    "    from stellargraph.mapper import GraphSAGENodeGenerator\n",
    "    from stellargraph.layer import GraphSAGE\n",
    "    print(f\"StellarGraph version: {sg.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"StellarGraph를 찾을 수 없습니다. 설치를 확인하세요.\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers, optimizers, losses, metrics, Model\n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"TensorFlow를 찾을 수 없습니다. 설치를 확인하세요.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a7c3ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "데이터 준비 결과 확인:\n",
      "  - node_feature_df shape: (5259857, 100)\n",
      "  - edges_df_filtered count: 1391357\n",
      "  - train_node_ids count: 4420308\n",
      "  - predict_node_ids count: 837944\n",
      "  - train_targets shape: (4420308, 62)\n",
      "  - num_classes: 62\n"
     ]
    }
   ],
   "source": [
    "# --- 이전 단계에서 생성된 변수 확인 (예시) ---\n",
    "# 이 변수들이 현재 메모리에 존재해야 합니다.\n",
    "print(f\"\\n데이터 준비 결과 확인:\")\n",
    "print(f\"  - node_feature_df shape: {node_feature_df.shape if 'node_feature_df' in locals() else 'Not Found'}\")\n",
    "print(f\"  - edges_df_filtered count: {len(edges_df_filtered) if 'edges_df_filtered' in locals() else 'Not Found'}\")\n",
    "print(f\"  - train_node_ids count: {len(train_node_ids) if 'train_node_ids' in locals() else 'Not Found'}\")\n",
    "print(f\"  - predict_node_ids count: {len(predict_node_ids) if 'predict_node_ids' in locals() else 'Not Found'}\")\n",
    "print(f\"  - train_targets shape: {train_targets.shape if 'train_targets' in locals() else 'Not Found'}\")\n",
    "print(f\"  - num_classes: {num_classes if 'num_classes' in locals() else 'Not Found'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4309d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. StellarGraph 객체 생성 ---\n",
      "노드 ID와 특징 인덱스 일치 확인됨.\n",
      "StellarGraph 생성을 위해 엣지 1391357개 준비 (Source, Target 컬럼만 사용).\n",
      "StellarGraph 객체 생성 완료.\n",
      "StellarGraph: Undirected multigraph\n",
      " Nodes: 5259857, Edges: 1391357\n",
      "\n",
      " Node types:\n",
      "  default: [5259857]\n",
      "    Features: float32 vector, length 100\n",
      "    Edge types: default-default->default\n",
      "\n",
      " Edge types:\n",
      "    default-default->default: [1391357]\n",
      "        Weights: all 1 (default)\n",
      "        Features: none\n"
     ]
    }
   ],
   "source": [
    "# --- 2. StellarGraph 객체 생성 ---\n",
    "print(\"\\n--- 2. StellarGraph 객체 생성 ---\")\n",
    "try:\n",
    "    # 노드 ID와 특징 인덱스 일치 확인 (이전 코드와 동일)\n",
    "    if 'df' in locals() and 'node_feature_df' in locals() and df.index.equals(node_feature_df.index):\n",
    "        print(\"노드 ID와 특징 인덱스 일치 확인됨.\")\n",
    "    else:\n",
    "        print(\"오류: df 또는 node_feature_df 변수를 찾을 수 없거나 인덱스가 일치하지 않습니다.\")\n",
    "        exit()\n",
    "\n",
    "    # **** 수정: StellarGraph에 필요한 엣지 컬럼만 선택 ****\n",
    "    if 'edges_df_filtered' in locals() and isinstance(edges_df_filtered, pd.DataFrame):\n",
    "        # 'Source'와 'Target' 컬럼만 선택하여 새로운 DataFrame 생성\n",
    "        edges_for_graph = edges_df_filtered[['Source', 'Target']].copy()\n",
    "        print(f\"StellarGraph 생성을 위해 엣지 {len(edges_for_graph)}개 준비 (Source, Target 컬럼만 사용).\")\n",
    "    else:\n",
    "        print(\"오류: 필터링된 엣지 데이터('edges_df_filtered')를 찾을 수 없습니다.\")\n",
    "        exit()\n",
    "    # ****************************************************\n",
    "\n",
    "    # 수정된 edges_for_graph 사용\n",
    "    G = sg.StellarGraph(nodes=node_feature_df, edges=edges_for_graph, source_column='Source', target_column='Target')\n",
    "\n",
    "    print(\"StellarGraph 객체 생성 완료.\")\n",
    "    print(G.info())\n",
    "\n",
    "except KeyError as e:\n",
    "    print(f\"StellarGraph 객체 생성 중 오류: 엣지 데이터프레임({e}) 또는 노드 데이터에 필요한 컬럼이 없습니다.\")\n",
    "    # print(f\"  엣지 컬럼: {edges_for_graph.columns.tolist() if 'edges_for_graph' in locals() else 'N/A'}\")\n",
    "    # print(f\"  노드 특징 컬럼: {node_feature_df.columns.tolist() if 'node_feature_df' in locals() else 'N/A'}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"StellarGraph 객체 생성 중 오류: {e}\")\n",
    "    exit()\n",
    "\n",
    "# (이하 모델 빌드 코드 계속)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdb7239a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. GraphSAGE 모델 빌드 ---\n",
      "훈련/검증 분할 시작 (stratify 없이 무작위 분할). 총 336199개 샘플.\n",
      "훈련/검증 데이터 생성 완료. 훈련 샘플: 302579, 검증 샘플: 33620\n",
      "\n",
      "--- 모델 빌드 완료 ---\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 10, 100)]    0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 50, 100)]    0           []                               \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 1, 100)]     0           []                               \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 1, 10, 100)   0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 10, 5, 100)   0           ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 1, 100)       0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 1, 10, 100)   0           ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 10, 100)      0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 10, 5, 100)   0           ['reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " mean_aggregator (MeanAggregato  multiple            3232        ['dropout_1[0][0]',              \n",
      " r)                                                               'dropout[0][0]',                \n",
      "                                                                  'dropout_3[0][0]',              \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 1, 10, 32)    0           ['mean_aggregator[1][0]']        \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 1, 32)        0           ['mean_aggregator[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 1, 10, 32)    0           ['reshape_2[0][0]']              \n",
      "                                                                                                  \n",
      " mean_aggregator_1 (MeanAggrega  (None, 1, 32)       1056        ['dropout_5[0][0]',              \n",
      " tor)                                                             'dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " reshape_3 (Reshape)            (None, 32)           0           ['mean_aggregator_1[0][0]']      \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 32)           0           ['reshape_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 62)           2046        ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,334\n",
      "Trainable params: 6,334\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "\n",
      "--- 4. 모델 학습 ---\n",
      "Epoch 1/5\n",
      "6052/6052 [==============================] - 165s 27ms/step - loss: 1.7369 - categorical_accuracy: 0.5303 - val_loss: 1.3089 - val_categorical_accuracy: 0.6189\n",
      "Epoch 2/5\n",
      "6052/6052 [==============================] - 169s 28ms/step - loss: 1.3368 - categorical_accuracy: 0.6107 - val_loss: 1.2238 - val_categorical_accuracy: 0.6419\n",
      "Epoch 3/5\n",
      "6052/6052 [==============================] - 164s 27ms/step - loss: 1.2901 - categorical_accuracy: 0.6214 - val_loss: 1.1891 - val_categorical_accuracy: 0.6497\n",
      "Epoch 4/5\n",
      "6052/6052 [==============================] - 152s 25ms/step - loss: 1.2682 - categorical_accuracy: 0.6276 - val_loss: 1.1722 - val_categorical_accuracy: 0.6557\n",
      "Epoch 5/5\n",
      "6052/6052 [==============================] - 160s 26ms/step - loss: 1.2526 - categorical_accuracy: 0.6311 - val_loss: 1.1517 - val_categorical_accuracy: 0.6589\n",
      "\n",
      "--- 모델 학습 완료 ---\n",
      "\n",
      "--- 5. 기간 7 노드 예측 ---\n",
      "16759/16759 [==============================] - 178s 11ms/step\n",
      "기간 7 노드 837944개에 대한 예측 완료.\n",
      "예측 결과 샘플 (Modularity Class / GraphSAGE 예측):\n",
      "                          modularity_class  modularity_class_GraphSAGE\n",
      "Id                                                                    \n",
      "53e99785b7602d9701f402db                -1                        44.0\n",
      "53e999fab7602d9702245390                -1                        44.0\n",
      "53e999ffb7602d9702248487                -1                        44.0\n",
      "53e99df7b7602d97026ba5aa                -1                        44.0\n",
      "53e9a20fb7602d9702b129f1                -1                        44.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# --- 3. GraphSAGE 모델 빌드 ---\n",
    "print(\"\\n--- 3. GraphSAGE 모델 빌드 ---\")\n",
    "if G is not None and 'train_node_ids' in locals() and len(train_node_ids) > 0 and 'train_targets' in locals():\n",
    "    # 하이퍼파라미터 (조절 필요)\n",
    "    batch_size = 50       # 메모리 상황에 따라 조절 (더 작게 시도 가능)\n",
    "    num_samples = [10, 5] # 샘플링할 이웃 수 (메모리/시간 영향 큼)\n",
    "    epochs = 5           # **주의: 초기 테스트를 위해 에폭 수를 매우 작게 설정 (실제로는 더 많이 필요)**\n",
    "    learning_rate = 1e-3\n",
    "    layer_sizes = [32, 32] # 레이어 유닛 수 (메모리 영향)\n",
    "\n",
    "    # 3.1. 데이터 제너레이터 생성\n",
    "    generator = GraphSAGENodeGenerator(G, batch_size, num_samples)\n",
    "\n",
    "    # 3.2. 훈련/검증 데이터 분할 및 flow 생성\n",
    "    # **** 중요: -1 레이블 제외 로직 적용된 변수 사용 ****\n",
    "    # 이전 단계에서 -1을 제외한 train_node_ids_filtered 와 train_labels_raw 를 사용해야 함\n",
    "    # (만약 이전 단계에서 해당 변수명을 사용했다면 아래 코드 수정 필요)\n",
    "\n",
    "    # 여기서는 -1 레이블 제외 코드가 바로 이전 단계에 없었으므로, 다시 적용합니다.\n",
    "    # (만약 이전 '6. 훈련 레이블 준비' 단계에서 이미 -1을 제외했다면 이 부분은 생략하고\n",
    "    #  train_test_split에 필터링된 ID와 레이블을 직접 사용해야 합니다.)\n",
    "    if 'train_labels_raw' in locals() and 'target_encoder' in locals():\n",
    "        valid_train_mask = (train_labels_raw != -1)\n",
    "        train_ids_for_split = train_node_ids[valid_train_mask]\n",
    "        train_targets_for_split = target_encoder.transform(train_labels_raw[valid_train_mask])\n",
    "        train_labels_raw_for_stratify = train_labels_raw[valid_train_mask] # stratify 용도\n",
    "\n",
    "        if len(train_ids_for_split) > 1: # 분할 가능한지 확인\n",
    "            print(f\"훈련/검증 분할 시작 (stratify 없이 무작위 분할). 총 {len(train_ids_for_split)}개 샘플.\")\n",
    "            # **** stratify 파라미터 제거 ****\n",
    "            train_set_ids, val_set_ids = train_test_split(\n",
    "                train_ids_for_split,  # -1 제외된 유효 훈련 노드 ID\n",
    "                test_size=0.1,        # 검증 세트 비율 (10%)\n",
    "                # stratify=train_labels_raw_for_stratify, # <-- 이 줄 제거 또는 주석 처리\n",
    "                random_state=42       # <-- 재현성을 위해 추가하는 것이 좋음 (선택 사항)\n",
    "            )\n",
    "\n",
    "            # **** 중요: 이후 타겟 생성 시에도 필터링된 ID 사용 ****\n",
    "            train_targets_split = target_encoder.transform(df.loc[train_set_ids, TARGET_COLUMN])\n",
    "            val_targets_split = target_encoder.transform(df.loc[val_set_ids, TARGET_COLUMN])\n",
    "\n",
    "            train_gen = generator.flow(train_set_ids, train_targets_split, shuffle=True)\n",
    "            val_gen = generator.flow(val_set_ids, val_targets_split)\n",
    "            print(f\"훈련/검증 데이터 생성 완료. 훈련 샘플: {len(train_set_ids)}, 검증 샘플: {len(val_set_ids)}\")\n",
    "        else:\n",
    "            print(\"오류: 유효한 훈련 데이터가 부족하여 분할/학습할 수 없습니다.\")\n",
    "            train_gen = None\n",
    "            val_gen = None\n",
    "    else:\n",
    "        print(\"오류: 훈련 레이블 정보가 없어 제너레이터를 생성할 수 없습니다.\")\n",
    "        train_gen = None\n",
    "        val_gen = None\n",
    "\n",
    "\n",
    "    # 3.4. Keras 모델 정의 (train_gen이 생성된 경우에만)\n",
    "    if train_gen is not None:\n",
    "        graphsage = GraphSAGE(\n",
    "            layer_sizes=layer_sizes, generator=generator, bias=True, dropout=0.3\n",
    "        )\n",
    "        x_inp, x_out = graphsage.in_out_tensors()\n",
    "        prediction = layers.Dense(units=num_classes, activation=\"softmax\")(x_out) # num_classes는 이전 단계에서 계산됨\n",
    "\n",
    "        model = Model(inputs=x_inp, outputs=prediction)\n",
    "\n",
    "        # 3.5. 모델 컴파일\n",
    "        model.compile(\n",
    "            optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss=losses.categorical_crossentropy,\n",
    "            metrics=[metrics.categorical_accuracy],\n",
    "        )\n",
    "        print(\"\\n--- 모델 빌드 완료 ---\")\n",
    "        print(model.summary()) # 모델 구조 출력\n",
    "\n",
    "\n",
    "        # --- 4. 모델 학습 ---\n",
    "        print(\"\\n--- 4. 모델 학습 ---\")\n",
    "        # 학습 데이터가 있고, 모델이 정의된 경우에만 학습 진행\n",
    "        history = model.fit(\n",
    "            train_gen,\n",
    "            epochs=epochs,          # **주의: 테스트를 위해 에폭 수를 작게 설정 (5)**\n",
    "            validation_data=val_gen,\n",
    "            verbose=1,              # 진행 상황을 더 자세히 보기 위해 1로 변경\n",
    "            shuffle=False\n",
    "        )\n",
    "        print(\"\\n--- 모델 학습 완료 ---\")\n",
    "        # (선택 사항) 학습 곡선 시각화\n",
    "        # sg.utils.plot_history(history)\n",
    "        # plt.show()\n",
    "\n",
    "\n",
    "        # --- 5. 기간 7 노드 예측 ---\n",
    "        print(\"\\n--- 5. 기간 7 노드 예측 ---\")\n",
    "        if 'predict_node_ids' in locals() and len(predict_node_ids) > 0 :\n",
    "            predict_gen = generator.flow(predict_node_ids)\n",
    "            p7_preds_probs = model.predict(predict_gen)\n",
    "\n",
    "            p7_pred_indices = np.argmax(p7_preds_probs, axis=1)\n",
    "            # target_label_map은 이전 단계에서 생성됨\n",
    "            p7_pred_labels = [target_label_map.get(idx, -99) for idx in p7_pred_indices]\n",
    "\n",
    "            # 예측 결과 저장 (df는 Id를 인덱스로 가짐)\n",
    "            df.loc[predict_node_ids, f'{TARGET_COLUMN}_GraphSAGE'] = p7_pred_labels\n",
    "\n",
    "            print(f\"기간 7 노드 {len(predict_node_ids)}개에 대한 예측 완료.\")\n",
    "            print(\"예측 결과 샘플 (Modularity Class / GraphSAGE 예측):\")\n",
    "            # 인덱스를 사용하여 샘플 출력\n",
    "            print(df.loc[predict_node_ids[:5], [TARGET_COLUMN, f'{TARGET_COLUMN}_GraphSAGE']])\n",
    "\n",
    "            # (선택 사항) 전체 df를 결과 포함하여 저장\n",
    "            # df.reset_index().to_csv('nodes_with_predictions.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "        else:\n",
    "            print(\"정보: 예측할 기간 7 노드가 없습니다.\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\n오류: 모델 학습에 필요한 데이터 제너레이터를 생성하지 못했습니다.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n오류: 모델 빌드 및 학습에 필요한 데이터/변수가 준비되지 않았습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df2db80",
   "metadata": {},
   "source": [
    "--- 3. GraphSAGE 모델 빌드 ---\n",
    "훈련/검증 분할 시작 (stratify 없이 무작위 분할). 총 336199개 샘플.\n",
    "훈련/검증 데이터 생성 완료. 훈련 샘플: 302579, 검증 샘플: 33620\n",
    "\n",
    "--- 모델 빌드 완료 ---\n",
    "Model: \"model\"\n",
    "__________________________________________________________________________________________________\n",
    " Layer (type)                   Output Shape         Param #     Connected to                     \n",
    "==================================================================================================\n",
    " input_2 (InputLayer)           [(None, 10, 100)]    0           []                               \n",
    "                                                                                                  \n",
    " input_3 (InputLayer)           [(None, 50, 100)]    0           []                               \n",
    "                                                                                                  \n",
    " input_1 (InputLayer)           [(None, 1, 100)]     0           []                               \n",
    "                                                                                                  \n",
    " reshape (Reshape)              (None, 1, 10, 100)   0           ['input_2[0][0]']                \n",
    "                                                                                                  \n",
    " reshape_1 (Reshape)            (None, 10, 5, 100)   0           ['input_3[0][0]']                \n",
    "                                                                                                  \n",
    " dropout_1 (Dropout)            (None, 1, 100)       0           ['input_1[0][0]']                \n",
    "                                                                                                  \n",
    " dropout (Dropout)              (None, 1, 10, 100)   0           ['reshape[0][0]']                \n",
    "                                                                                                  \n",
    " dropout_3 (Dropout)            (None, 10, 100)      0           ['input_2[0][0]']                \n",
    "                                                                                                  \n",
    " dropout_2 (Dropout)            (None, 10, 5, 100)   0           ['reshape_1[0][0]']              \n",
    "                                                                                                  \n",
    " mean_aggregator (MeanAggregato  multiple            3232        ['dropout_1[0][0]',              \n",
    " r)                                                               'dropout[0][0]',                \n",
    "                                                                  'dropout_3[0][0]',              \n",
    "                                                                  'dropout_2[0][0]']              \n",
    "                                                                                                  \n",
    " reshape_2 (Reshape)            (None, 1, 10, 32)    0           ['mean_aggregator[1][0]']        \n",
    "                                                                                                  \n",
    " dropout_5 (Dropout)            (None, 1, 32)        0           ['mean_aggregator[0][0]']        \n",
    "                                                                                                  \n",
    " dropout_4 (Dropout)            (None, 1, 10, 32)    0           ['reshape_2[0][0]']              \n",
    "                                                                                                  \n",
    " mean_aggregator_1 (MeanAggrega  (None, 1, 32)       1056        ['dropout_5[0][0]',              \n",
    " tor)                                                             'dropout_4[0][0]']              \n",
    "                                                                                                  \n",
    " reshape_3 (Reshape)            (None, 32)           0           ['mean_aggregator_1[0][0]']      \n",
    "                                                                                                  \n",
    " lambda (Lambda)                (None, 32)           0           ['reshape_3[0][0]']              \n",
    "                                                                                                  \n",
    " dense (Dense)                  (None, 62)           2046        ['lambda[0][0]']                 \n",
    "                                                                                                  \n",
    "==================================================================================================\n",
    "Total params: 6,334\n",
    "Trainable params: 6,334\n",
    "Non-trainable params: 0\n",
    "__________________________________________________________________________________________________\n",
    "None\n",
    "\n",
    "--- 4. 모델 학습 ---\n",
    "Epoch 1/5\n",
    "6052/6052 [==============================] - 165s 27ms/step - loss: 1.7369 - categorical_accuracy: 0.5303 - val_loss: 1.3089 - val_categorical_accuracy: 0.6189\n",
    "Epoch 2/5\n",
    "6052/6052 [==============================] - 169s 28ms/step - loss: 1.3368 - categorical_accuracy: 0.6107 - val_loss: 1.2238 - val_categorical_accuracy: 0.6419\n",
    "Epoch 3/5\n",
    "6052/6052 [==============================] - 164s 27ms/step - loss: 1.2901 - categorical_accuracy: 0.6214 - val_loss: 1.1891 - val_categorical_accuracy: 0.6497\n",
    "Epoch 4/5\n",
    "6052/6052 [==============================] - 152s 25ms/step - loss: 1.2682 - categorical_accuracy: 0.6276 - val_loss: 1.1722 - val_categorical_accuracy: 0.6557\n",
    "Epoch 5/5\n",
    "6052/6052 [==============================] - 160s 26ms/step - loss: 1.2526 - categorical_accuracy: 0.6311 - val_loss: 1.1517 - val_categorical_accuracy: 0.6589\n",
    "\n",
    "--- 모델 학습 완료 ---\n",
    "\n",
    "--- 5. 기간 7 노드 예측 ---\n",
    "16759/16759 [==============================] - 178s 11ms/step\n",
    "기간 7 노드 837944개에 대한 예측 완료.\n",
    "예측 결과 샘플 (Modularity Class / GraphSAGE 예측):\n",
    "                          modularity_class  modularity_class_GraphSAGE\n",
    "Id                                                                    \n",
    "53e99785b7602d9701f402db                -1                        44.0\n",
    "53e999fab7602d9702245390                -1                        44.0\n",
    "53e999ffb7602d9702248487                -1                        44.0\n",
    "53e99df7b7602d97026ba5aa                -1                        44.0\n",
    "53e9a20fb7602d9702b129f1                -1                        44.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb55fea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 5. 기간 7 노드 예측 ---\n",
      "기간 7 노드 837944개에 대한 예측 제너레이터 생성 중...\n",
      "예측 제너레이터 생성 완료.\n",
      "모델 예측 수행 중 (시간이 걸릴 수 있습니다)...\n",
      "16759/16759 [==============================] - 166s 10ms/step\n",
      "모델 예측 완료.\n",
      "예측 결과를 'modularity_class_GraphSAGE' 컬럼에 저장 중...\n",
      "기간 7 노드 837944개에 대한 예측 완료.\n",
      "예측 결과 샘플 (원본 Modularity Class / GraphSAGE 예측):\n",
      "                          modularity_class  modularity_class_GraphSAGE\n",
      "Id                                                                    \n",
      "53e99785b7602d9701f402db                -1                        44.0\n",
      "53e999fab7602d9702245390                -1                        44.0\n",
      "53e999ffb7602d9702248487                -1                        44.0\n",
      "53e99df7b7602d97026ba5aa                -1                        44.0\n",
      "53e9a20fb7602d9702b129f1                -1                        44.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd # 혹시 모르니 임포트 확인\n",
    "\n",
    "# --- 5. 기간 7 노드 예측 ---\n",
    "print(\"\\n--- 5. 기간 7 노드 예측 ---\")\n",
    "\n",
    "# 필요한 변수들이 존재하는지 확인 (더욱 안전하게)\n",
    "required_vars_for_predict = ['model', 'generator', 'predict_node_ids', 'target_label_map', 'df', 'TARGET_COLUMN']\n",
    "if not all(var in locals() or var in globals() for var in required_vars_for_predict):\n",
    "    print(\"오류: 예측에 필요한 변수 중 일부가 정의되지 않았습니다.\")\n",
    "    print(f\"필요 변수: {required_vars_for_predict}\")\n",
    "    # 필요한 경우 여기서 중단 (exit() 등)\n",
    "\n",
    "# 예측 대상 노드가 있고, 학습된 모델과 제너레이터가 있을 경우 진행\n",
    "elif len(predict_node_ids) > 0 and 'model' in locals() and 'generator' in locals():\n",
    "    try:\n",
    "        # 5.1. 예측용 데이터 제너레이터 생성\n",
    "        #      predict_node_ids의 ID들이 generator 생성 시 사용된 G 객체에 있는지 확인 필요\n",
    "        #      (일반적으로 df에서 추출했으므로 문제 없음)\n",
    "        print(f\"기간 7 노드 {len(predict_node_ids)}개에 대한 예측 제너레이터 생성 중...\")\n",
    "        predict_gen = generator.flow(predict_node_ids)\n",
    "        print(\"예측 제너레이터 생성 완료.\")\n",
    "\n",
    "        # 5.2. 모델 예측 수행 -> 클래스별 확률 얻기\n",
    "        print(\"모델 예측 수행 중 (시간이 걸릴 수 있습니다)...\")\n",
    "        p7_preds_probs = model.predict(predict_gen)\n",
    "        print(\"모델 예측 완료.\")\n",
    "\n",
    "        # 5.3. 가장 높은 확률의 클래스 인덱스 찾기\n",
    "        p7_pred_indices = np.argmax(p7_preds_probs, axis=1)\n",
    "\n",
    "        # 5.4. 클래스 인덱스를 원래 Modularity Class 레이블로 변환\n",
    "        #      target_label_map은 이전 '6. 훈련 레이블 준비' 단계에서 생성됨\n",
    "        p7_pred_labels = [target_label_map.get(idx, -99) for idx in p7_pred_indices] # 매핑 실패 시 -99\n",
    "\n",
    "        # 5.5. 예측 결과 저장 (df는 Id를 인덱스로 가지고 있어야 함)\n",
    "        predict_col_name = f'{TARGET_COLUMN}_GraphSAGE'\n",
    "        print(f\"예측 결과를 '{predict_col_name}' 컬럼에 저장 중...\")\n",
    "        if df.index.name == 'Id': # df의 인덱스가 'Id'인지 재확인\n",
    "             # predict_node_ids 와 p7_pred_labels 길이가 같은지 확인 (디버깅용)\n",
    "             if len(predict_node_ids) == len(p7_pred_labels):\n",
    "                  df.loc[predict_node_ids, predict_col_name] = p7_pred_labels\n",
    "                  print(f\"기간 7 노드 {len(predict_node_ids)}개에 대한 예측 완료.\")\n",
    "                  print(\"예측 결과 샘플 (원본 Modularity Class / GraphSAGE 예측):\")\n",
    "                  # 인덱스를 사용하여 샘플 출력\n",
    "                  print(df.loc[predict_node_ids[:5], [TARGET_COLUMN, predict_col_name]])\n",
    "             else:\n",
    "                  print(f\"오류: 예측 대상 ID 수({len(predict_node_ids)})와 예측된 레이블 수({len(p7_pred_labels)})가 다릅니다!\")\n",
    "        else:\n",
    "             print(\"오류: 예측 결과를 저장하기 위해 df의 인덱스가 'Id'여야 합니다.\")\n",
    "\n",
    "\n",
    "        # (선택 사항) 전체 df를 결과 포함하여 저장\n",
    "        # try:\n",
    "        #     save_path = 'nodes_with_predictions.csv'\n",
    "        #     df.reset_index().to_csv(save_path, index=False, encoding='utf-8-sig')\n",
    "        #     print(f\"예측 결과 포함된 전체 노드 데이터 저장 완료: {save_path}\")\n",
    "        # except Exception as e_save:\n",
    "        #      print(f\"결과 저장 중 오류 발생: {e_save}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"예측 중 오류 발생: {e}\")\n",
    "\n",
    "elif len(predict_node_ids) == 0:\n",
    "    print(\"정보: 예측할 기간 7 노드가 없습니다.\")\n",
    "else:\n",
    "    print(\"오류: 모델이 학습되지 않았거나 필요한 변수가 없어 예측을 수행할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "926d865b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Degree</th>\n",
       "      <th>Label</th>\n",
       "      <th>citations</th>\n",
       "      <th>indegree</th>\n",
       "      <th>keywords</th>\n",
       "      <th>level</th>\n",
       "      <th>modularity_class</th>\n",
       "      <th>outdegree</th>\n",
       "      <th>research_fields</th>\n",
       "      <th>venue</th>\n",
       "      <th>year</th>\n",
       "      <th>modularity_class_GraphSAGE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53a725db20f7420be8b5bfc6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Deductive Algorithmic Knowledge</td>\n",
       "      <td>37.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[deductive system, fixed deductive system, pro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>Journal of Logic and Computation</td>\n",
       "      <td>2004</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53a725db20f7420be8b5bff3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Improving exact algorithms for MAX-2-SAT</td>\n",
       "      <td>26.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[new variable, clause-to-variable ratio, graph...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>Annals of Mathematics and Artificial Intelligence</td>\n",
       "      <td>2004</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53a725db20f7420be8b5bffb</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Learning via finitely many queries</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[new query inference model, Boolean query, exp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>Annals of Mathematics and Artificial Intelligence</td>\n",
       "      <td>2004</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53a725db20f7420be8b5c00f</th>\n",
       "      <td>NaN</td>\n",
       "      <td>A framework for sequential planning in multi-a...</td>\n",
       "      <td>501.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[belief state, value function, approximate bel...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Mathematical optimization, Partially observab...</td>\n",
       "      <td>Journal of Artificial Intelligence Research</td>\n",
       "      <td>2004</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53a727dc20f7420be8b9eed7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Evaluation of Prediction Methods Applied to an...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Basurde Spanish dialogue system, dialogue sta...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Morpheme, Hit ratio, Computer science, Keystr...</td>\n",
       "      <td>TSD '02 Proceedings of the 5th International C...</td>\n",
       "      <td>2002</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Degree  \\\n",
       "Id                                 \n",
       "53a725db20f7420be8b5bfc6     NaN   \n",
       "53a725db20f7420be8b5bff3     NaN   \n",
       "53a725db20f7420be8b5bffb     NaN   \n",
       "53a725db20f7420be8b5c00f     NaN   \n",
       "53a727dc20f7420be8b9eed7     NaN   \n",
       "\n",
       "                                                                      Label  \\\n",
       "Id                                                                            \n",
       "53a725db20f7420be8b5bfc6                    Deductive Algorithmic Knowledge   \n",
       "53a725db20f7420be8b5bff3           Improving exact algorithms for MAX-2-SAT   \n",
       "53a725db20f7420be8b5bffb                 Learning via finitely many queries   \n",
       "53a725db20f7420be8b5c00f  A framework for sequential planning in multi-a...   \n",
       "53a727dc20f7420be8b9eed7  Evaluation of Prediction Methods Applied to an...   \n",
       "\n",
       "                          citations  indegree  \\\n",
       "Id                                              \n",
       "53a725db20f7420be8b5bfc6       37.0       NaN   \n",
       "53a725db20f7420be8b5bff3       26.0       NaN   \n",
       "53a725db20f7420be8b5bffb        0.0       NaN   \n",
       "53a725db20f7420be8b5c00f      501.0       NaN   \n",
       "53a727dc20f7420be8b9eed7        5.0       NaN   \n",
       "\n",
       "                                                                   keywords  \\\n",
       "Id                                                                            \n",
       "53a725db20f7420be8b5bfc6  [deductive system, fixed deductive system, pro...   \n",
       "53a725db20f7420be8b5bff3  [new variable, clause-to-variable ratio, graph...   \n",
       "53a725db20f7420be8b5bffb  [new query inference model, Boolean query, exp...   \n",
       "53a725db20f7420be8b5c00f  [belief state, value function, approximate bel...   \n",
       "53a727dc20f7420be8b9eed7  [Basurde Spanish dialogue system, dialogue sta...   \n",
       "\n",
       "                         level  modularity_class  outdegree  \\\n",
       "Id                                                            \n",
       "53a725db20f7420be8b5bfc6   NaN                -1        NaN   \n",
       "53a725db20f7420be8b5bff3   NaN                -1        NaN   \n",
       "53a725db20f7420be8b5bffb   NaN                -1        NaN   \n",
       "53a725db20f7420be8b5c00f   NaN                -1        NaN   \n",
       "53a727dc20f7420be8b9eed7   NaN                -1        NaN   \n",
       "\n",
       "                                                            research_fields  \\\n",
       "Id                                                                            \n",
       "53a725db20f7420be8b5bfc6                                                 []   \n",
       "53a725db20f7420be8b5bff3                                                 []   \n",
       "53a725db20f7420be8b5bffb                                                 []   \n",
       "53a725db20f7420be8b5c00f  [Mathematical optimization, Partially observab...   \n",
       "53a727dc20f7420be8b9eed7  [Morpheme, Hit ratio, Computer science, Keystr...   \n",
       "\n",
       "                                                                      venue  \\\n",
       "Id                                                                            \n",
       "53a725db20f7420be8b5bfc6                   Journal of Logic and Computation   \n",
       "53a725db20f7420be8b5bff3  Annals of Mathematics and Artificial Intelligence   \n",
       "53a725db20f7420be8b5bffb  Annals of Mathematics and Artificial Intelligence   \n",
       "53a725db20f7420be8b5c00f        Journal of Artificial Intelligence Research   \n",
       "53a727dc20f7420be8b9eed7  TSD '02 Proceedings of the 5th International C...   \n",
       "\n",
       "                          year  modularity_class_GraphSAGE  \n",
       "Id                                                          \n",
       "53a725db20f7420be8b5bfc6  2004                         NaN  \n",
       "53a725db20f7420be8b5bff3  2004                         NaN  \n",
       "53a725db20f7420be8b5bffb  2004                         NaN  \n",
       "53a725db20f7420be8b5c00f  2004                         NaN  \n",
       "53a727dc20f7420be8b9eed7  2002                         NaN  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a4ee867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 1. 전체 데이터프레임 저장 ---\n",
      "'C:/Project/RAG연구동향분석/250409/Final_Results\\nodes_with_predictions_full.csv' 경로에 전체 노드 데이터(예측 포함) 저장 시작...\n",
      "전체 데이터 저장 완료: C:/Project/RAG연구동향분석/250409/Final_Results\\nodes_with_predictions_full.csv\n",
      "\n",
      "--- 2. 기간 7 데이터 필터링 및 저장 ---\n",
      "'year' 컬럼 기준으로 2020~2022 데이터 필터링 중...\n",
      "기간 7 데이터 837944개 필터링 완료.\n",
      "'C:/Project/RAG연구동향분석/250409/Final_Results\\nodes_period7_predictions.csv' 경로에 기간 7 데이터 저장 시작...\n",
      "기간 7 데이터 저장 완료: C:/Project/RAG연구동향분석/250409/Final_Results\\nodes_period7_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np # 혹시 모를 numpy 사용을 위해\n",
    "\n",
    "# --- 사용자 설정: 저장 경로 및 파일 이름 ---\n",
    "# 저장할 디렉토리를 지정합니다.\n",
    "output_dir_final = \"C:/Project/RAG연구동향분석/250409/Final_Results\"\n",
    "# 전체 데이터 파일 이름\n",
    "full_df_filename = \"nodes_with_predictions_full.csv\"\n",
    "# 기간 7 데이터 파일 이름\n",
    "period7_df_filename = \"nodes_period7_predictions.csv\"\n",
    "\n",
    "output_path_full = os.path.join(output_dir_final, full_df_filename)\n",
    "output_path_p7 = os.path.join(output_dir_final, period7_df_filename)\n",
    "\n",
    "# 저장할 디렉토리가 없으면 생성\n",
    "os.makedirs(output_dir_final, exist_ok=True)\n",
    "\n",
    "# 필요한 컬럼 이름 확인\n",
    "YEAR_COLUMN = 'year' # 실제 연도 컬럼 이름 확인\n",
    "# -------------------------------------------\n",
    "\n",
    "# --- 1. 전체 df 저장 ---\n",
    "print(f\"\\n--- 1. 전체 데이터프레임 저장 ---\")\n",
    "if 'df' in locals() and isinstance(df, pd.DataFrame):\n",
    "    try:\n",
    "        print(f\"'{output_path_full}' 경로에 전체 노드 데이터(예측 포함) 저장 시작...\")\n",
    "        # 'Id' 인덱스를 컬럼으로 변환 후 저장, 기본 RangeIndex는 저장 안 함\n",
    "        df.reset_index().to_csv(output_path_full, index=False, encoding='utf-8-sig')\n",
    "        print(f\"전체 데이터 저장 완료: {output_path_full}\")\n",
    "    except Exception as e:\n",
    "        print(f\"전체 데이터 저장 중 오류 발생: {e}\")\n",
    "else:\n",
    "    print(\"오류: 저장할 'df' 데이터프레임을 찾을 수 없습니다.\")\n",
    "\n",
    "\n",
    "# --- 2. 기간 7 데이터 필터링 및 저장 ---\n",
    "print(f\"\\n--- 2. 기간 7 데이터 필터링 및 저장 ---\")\n",
    "if 'df' in locals() and isinstance(df, pd.DataFrame) and YEAR_COLUMN in df.columns:\n",
    "    try:\n",
    "        # 기간 7 정의\n",
    "        period7_start = 2020\n",
    "        period7_end = 2022\n",
    "\n",
    "        # 기간 7 데이터 필터링\n",
    "        print(f\"'{YEAR_COLUMN}' 컬럼 기준으로 {period7_start}~{period7_end} 데이터 필터링 중...\")\n",
    "        # df의 year 컬럼 타입을 다시 확인 (정수형이어야 함)\n",
    "        if not pd.api.types.is_integer_dtype(df[YEAR_COLUMN]):\n",
    "             print(f\"  경고: '{YEAR_COLUMN}' 컬럼이 정수형이 아닙니다. 변환 시도...\")\n",
    "             # 이미 이전 단계에서 정수 변환 했으므로 보통 문제 없음\n",
    "             df[YEAR_COLUMN] = pd.to_numeric(df[YEAR_COLUMN], errors='coerce').fillna(-1).astype(int)\n",
    "\n",
    "        period7_mask = (df[YEAR_COLUMN] >= period7_start) & (df[YEAR_COLUMN] <= period7_end)\n",
    "        df_period7 = df[period7_mask].copy() # .copy() 사용하여 SettingWithCopyWarning 방지\n",
    "\n",
    "        print(f\"기간 7 데이터 {len(df_period7)}개 필터링 완료.\")\n",
    "\n",
    "        if not df_period7.empty:\n",
    "            # 기간 7 데이터 저장\n",
    "            print(f\"'{output_path_p7}' 경로에 기간 7 데이터 저장 시작...\")\n",
    "            # 'Id' 인덱스를 컬럼으로 변환 후 저장\n",
    "            df_period7.reset_index().to_csv(output_path_p7, index=False, encoding='utf-8-sig')\n",
    "            print(f\"기간 7 데이터 저장 완료: {output_path_p7}\")\n",
    "        else:\n",
    "            print(\"정보: 기간 7에 해당하는 데이터가 없어 파일을 저장하지 않습니다.\")\n",
    "\n",
    "    except KeyError:\n",
    "         print(f\"오류: '{YEAR_COLUMN}' 컬럼이 df에 없습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"기간 7 데이터 처리 또는 저장 중 오류 발생: {e}\")\n",
    "\n",
    "elif not ('df' in locals() and isinstance(df, pd.DataFrame)):\n",
    "     print(\"오류: 'df' 데이터프레임을 찾을 수 없어 기간 7 데이터를 처리할 수 없습니다.\")\n",
    "else:\n",
    "     print(f\"오류: '{YEAR_COLUMN}' 컬럼이 df에 없어 기간 7 데이터를 처리할 수 없습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sage_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
